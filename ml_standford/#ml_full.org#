
* Machine learning by Andrew Ng on cousere

Learning Algorithms is important, but more important
to know how to convey them into practical problem.
                               -- andrew Ng 
** Introduction
							   
*** Before the course

The AI dream of someday building machines as intelligent as people.
And many AI researchers believe the best one to towards that goal
is through *learning algorithms mimic how the humans' brains learn*.
Just knowing the algorithms and knowing the math *isn't enough* if
u don't know how to actually get this stuff to work on problems. the
ranges of machine learning applications:

+ Database mining

  Large data-sets from growth of web are very useful to humans for analysis.
  E.g:
  + Web Click Data

    analyzing(machine learning)  web click data help those internet companies
    know and serve the users better

  + Medical Records

    Using machine learning to analyzing the records and turn them to medical
    knowledge help human to know diseases better is increasing necessary in
    medical field.

  + Computational Biology

    Machine learning algorithms figure out what collected information about
    the DNA sequences means to human.

  + Engineering field

    architecture field is still unexplored so much :)

+ Applications which cant be programed by hand
  
  We cant program to solve problems but we could make robotic program
  to learn how to solve the program by itself. E.g.
  + Autonomous helicopter

    It's hard to program to drive a helicopter but we could make a computer
    program to learn how to fly the helicopter by itself.

  + N-L-P (Nature Language Processing)
  + Computer Vision
  + Self-customizing programs
    
    E.g. Amazon product recommendations, the computer program learns and
    customizes itself to specific customers based on their preferences.

  + Understanding human learning (brain, real AI)
*** What is machine learning?
**** The old definition as 1959 by Arthur Samuel. 
     + Informal and older definition:

       Field of study that gives computers the ability to learn without being
       explicitly programmed e.g.

       samuel built a checker program(robot) to play thousands of games
       to "understand" what kind of boards to win or lose, and eventually learn
       how to play checker. Although Samuel was not a good a checker himself but
       his "robot" make him better checker.
**** The modern definition by Tom Mitchell
     + Well-posed Learning Problem:

       A computer program is said to learn from experience E with the respect
       to some task T and some performance on T,
       improved with experience E.(T & E & P) e.g.

       G-mail spam filter robot classifies the spam and non-spam emails (T) automatically.
       its E is whether the users label some specific emails are spam or not.
       its P is the number of fraction of emails correctly classified as spams or
       not spams.
**** The main machine learning algorithms:
     + The main two types
       1. Supervised learning
       2. Unsupervised learning
     + Others
       1. Reinforcement learning
       2. Recommend systems

*** Supervised Learning Introduction

    Problem was provided with the so-called "right-answer-sample-data"
    (term: trianing set) , supervised learning problem focus more on 
    prediction by "learning" from training set.

**** Linear Regression 						    

     To predict *the continuous values* based on the sample data("right-answer")
     (the saying continuous value here, I guess, is because what the prediction
     's value must be one value of some specific continuum.
     E.g. The house prices sample:

     file:r/0010.jpg

     file:r/0011.jpg
     (the car keep trying to predict continueous values of steering directions
     to drive itself)
**** Classification(logistic Regression)

     To predict the *discrete value* based on the training set
     E.g.

     file:r/0030.jpg

     In the example, algoritmn analyse and classify the datasets
     into two types (magnigetn or benign), so we could predict a
     specific user-provided data weather magnigetn or benign.

     Tips: in some questions, the discrete values may more than 2
     categories e.g. magnigetn1, magnigetn2, magnigtn3 .., benign
     (Multi-class classification).
     And may be more than 2 features (> 2-dimentions), e.g.considering
     tumor position.. as well  and even maybe inifinit features in
     some examples (obviously, we don't have infinit memory to store 
     this data), fortunally, the support of vector machine may give
     some help.
     
*** Unsupervised Learning Introduction Clustering Algorithm

    This is no any known categories provided in advance versus the
    classification in supervised learning. Instead, the typical one
    about unsupervised learning algorighm is to analyse the dataset
    and divide them into different clusters. (i.e. try to find the
    structure or inner category of the dataset hidding inside)
    Let's see some examples in our life.
     
    + Group the same new from different source

      [[file:r/0040.jpg]]

    + Cluster gene-block have different influences on human
  
      file:r/0050.jpg 
      
    + Sounds seperation

      file:r/0070.jpg

    + More examples

      file:r/0060.jpg
    
           
** Linear Regression With One Variable(focusing on the housing prices example)

*** linear regression with one variable model(abbreviate to <<LR1V>>)
(illustrating with the housing prices example)
Definition of Multivariate Linear Regression (abbreviate to <<MLR>>):
In this supervised learning model, we predict the outputs(the housing price)
through a linear hypothesis function which has n variables.
(h = (theta0 + theta1 * x0 + theta2 * x1 * theta3 * x2....))

Definition of [[LR1V]]:
one variable for h in LRM (e.g the price u friend gived u for prediction)
**** h for [[LR1V]] :x

     \[
     h_\theta(x) = \theta_{0} + \theta_{1} \times x
     \]

     [[file:r/0080.jpg][schematic graph]]

**** The cost function for [[LR1V]]
     Different theta0, theta1 will bring different hypothesis function (figure),
     which "fit" the training set in different levels, and we need to define a
     *cost function* to quantize the "fit" level or the quality of h.
     There are many cost function, but the square error function is probably
     the most commonly used one for the cost function of linear regression
     problem.     
***** <<Squared error funciton>> as cost function

      \[
      J(\theta_{0}, \theta_{1}) = \frac{1}{2m} \sum_{i=1}^m(h_{\theta}(x_{i}) - y_{i})^{2}
      \]

      actually, J(theta0, theta1) =
      a * theta0^2 + b * theta1 ^ 2 + c * theta0 * theta1 + d (bow-shape graph)
***** The goal for the cost function
      Obviously, we need the cost as less as possible to fit or close the 
      training set more. so our goal is to:
      *find the (theta0, theta1) to minimize the cost function*

      Squared error function' graph of the housing prices example
      comes to: [[file:r/0090.jpg][J(theta0, theta1)]], And it's more convinient to use
      the corresponding  [[file:r/0100.jpg][contour graph]] to replace it for our analysis.
      each (theta0, theta1) in the same ellipse path has the same. 
      J(theta0, theta1) value. So we could tell the center point is 
      the one minimizing the J and that's what we want.
      
***** Getting the goal by *Gradient Descent Function<<GFD>> Algorithm* <<GFDA>>
      In the class, Andrew don't explain why we just calculate the 
      minimum (theta0, theta1) for the cost function J. But I guess,
      It's not easy or efficient to calculate it, even for some 
      complicated J. and instead we could use GFDA to get the suboptimul
      solution(or local minimum). GFDA is a more general algorithm and 
      is used not only in linear regression but other parts in ML.
      GFDA is a abstract algorithm to *minimize arbitrary functions* include 
      cost-functions and other function with various variables.
      (?: Is andrew's saying rigorous? actually we just get subminimal, can we
      say minimize some function?)
      notice that J here refers to arbitrary general function J(theta0, theta1,
      theta2, theta3 ....), but we take 2 parameters for simplicity. 

******* Batch Gradient Descent
	"Batch" : Each step of gradient descent uses all the training examples.
	
*******  The procedure of GFDA:
         + Start with some specific theta0, theta1
         + Keep changing theta0, theta1 to "reduce" J(theta0, theta1)
	   use the following gradient descent function:
	   \[
	   \theta_{j} := \theta_{j} - \alpha \times \frac{\partial J(\theta_{0},\theta{1})}{\partial \theta_{j}}
	   \ (for\ j\ = 0\ and\ j\ =\ 1)
	   \]
	   untill theta, theta1, changes sligtly.
           *Warning:* 
	   theta0, theta1 must be *updated simultaneously* that:	 
	   $tmp0 := \theta_{0} - \alpha \times \frac{\partial J(\theta_{0},\theta{1})}{\partial \theta_{0}}$
	   $tmp1 := \theta_{1} - \alpha \times \frac{\partial J(\theta_{0},\theta{1})}{\partial \theta_{1}}$
	   $\theta_{0}\ :=\ tmp0$
	   $\theta_{1}\ :=\ tmp1$ 
******* Simple analysis of [[GFDA]]
	- The running procedure looks like:
	 [[file:r/0120.jpg][ starting from (0.7, 0.4)]]
	  [[file:r/0110.jpg][starting from (0.6, 0.4)]]
	- The obvious property of [[GFDA]]:
	  + subopimum or global optimum
	    if the target function is Error Square Function J(theta0, theta1)
	    then different starting points end up with same global optimum 
	    becuase J has bow-shape like function graph.
	    but if our target function is like [[file:r/0110.jpg][starting from (0.6, 0.4)]], 
	    different starting points may end up with different suboptimums.

	  + If the first few iterations of gradient descent cause 
	    f(theta0,theta1) to increase rather than decrease, then the most
	    likely cause is that we have set the learning rate α to too large
	    a value.
        - Why [[GFDA]] works
	  ?. here just explaining J has less than 2 variables, but how about 
	  more than 2.
	  we could regard one variable as active but the other one is fixed.
	  then the graph would truns to be a curve with gradient, which
	  aligned to the (the other variable, J) planar.
	  pictures are worth a thousand words:
	  [[file:r/0130.jpg][the "curve" for different theta]]
	  [[file:r/0140.jpg][for each "curve"]]
	  [[file:r/0150.jpg]["self-adjustment" when getting to the local minimal]]
	  [[file:r/0160.jpg][learning rate need not too large]]
******* applying GFDA to cost function
       so even if our target is the [[square error function]], we could go further
       for GFDA formular:

       \[
       \theta_{0} := \theta_{0} - \frac{\alpha}{m} \times \frac{\partial (h_{\theta}(x_{i}) - y_{i} )}{\partial \theta_{j}}
       \]
       
       \[
       \theta_{1} := \theta_{1} - \frac{\alpha}{m} \times \frac{\partial (h_{\theta}(x_{i}) - y_{i} )}{\partial \theta_{j}} \times x_{i}
       \]

       of course we are emplying "Batch" Gradient Descent here 
                    

** Linear Regression With Multiple Variables (Multative Linear Regression)
   
*** multiple features(variables)
    In the previous example of house price prediction, we just have the size
    (x, one variable, input) which constrains our price(y, the ouput).But here
    we will deal with multiple variables constrains the price together.

         $x_{1}$                   $x_2$                $x_{3}$             $x_{4}$           $x_{5}$   
    | Size(feets) | Number of bedrooms | Number of floors | Age of home |   Price |
    |-------------+--------------------+------------------+-------------+---------|
    |        2104 |                  5 |                1 |          45 |     460 |
    |        1416 |                  3 |                2 |          40 |     232 |
    |        1535 |                  3 |                2 |          30 |     315 |
    |           . |                  . |                . |           . |       . |

**** the notations
     1. n
        number of features i.g. n = 4 here
     2. $x^{(i)}$ 
        variable of i-th training set (the i-th row of training example)
        i.g.  $x^{(3)} : \begin{vmatrix} 1535 \\  3 \\  2 \\  30 \\  \end{vmatrix}$
     4. $x_{j}$
	value of feature j in one specific row of trianing set (do not care
	about other rows of trianing set)
     3. $x_{j}^{(i)}$
        value of feature j in $x^{(i)}$
        i.g. $x_{2}^{(1)}$ equals 5
	
**** the hypothesis of [[MLR]]
     $h_{\theta}(x) = \theta ^ {T} \times X^{i} = \theta_0 \times x_0 + \theta_1 * x_1 + \theta_2 * x_{2} + \theta_3 * x_{3} + \theta_4 * x_{4}$
           $= \sum_{j=0}^{n}(\theta_{j} \times x_{j})$
     $x_{0} = 1$
     θ : (1, n+1) vector

**** the cost function of h for [[MLR]]

     $J(\theta) = J(\theta_{0}, \theta{1}, \theta_{2}, \theta_{3}, \theta_{4}, .. , \theta_{n}) = \frac{1}{2m} \sum_{i=1}^m(h_{\theta}(x^{i}) - y^i)^{2} = \frac{1}{2m}(X\theta - y)^{T}(X\theta - y)$
     
**** gradient descend algorithm to minimize J(theta) for [[MLR]]
      
      $\theta_j := \theta_j - \alpha \times \frac{\partial J(\theta)}{\partial \theta_{j}} = \theta_j - \frac{\alpha}{m} \sum_{i=1}^m(h_{\theta}(x^{i}) - y^i)x_{j}^{(i)} \ \  (0 < j < n)$ 
      we could also employ vectorization to facilitate our computation *for each iteration* as:
      $\theta := \theta - \frac{\alpha}{m} \begin{vmatrix} MatError \times X_{0} \\  MatError \times X_{1} \\  MatError \times X_{2} \\  . \\  . \\   MatError \times X_{n}  \end{vmatrix}$
      MatError is a constant vector during all computing.
      revised here 
#+begin_src octave
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha
% Initialize some useful values
m = length(y); % number of training examples
n = size(X,2); % number of features + 1
J_history = zeros(num_iters, 1);
mat_delta = zeros(n, 1);  % we call it "gradient"
n = size(X, 2); %% feather dimension
for iter = 1:num_iters
    mat_error = X * theta - y; % (m, 1) vecor for error value
    for _k = 1:n
        x_k_clm = X(:, _k); % the k-th colomn of X, (m, 1) vector
        mat_delta(_k) =  transpose(x_k_clm) * mat_error;
    end    
    theta = theta - (alpha / m) .* mat_delta;
    J_history(iter) = computeCost(X, y, theta);
end
end
#+end_src

*** make sure gradient descent is working correctly
**** feature scaling
make sure features on a similar scales:  $-1 <=x_{j} <=1$
the gradient descend algorithm works in a very slow and zigzagging way to get the
local minimum if the contour of J comes to be tall and skinny. And this problem 
could solve by scaling all features to similar scales.
[[file:r/0170.jpg][future scaling]]

tips: when u are trying to do the prediction with your theta form gradient algorithm,
but remember,before that, u need employ the mean normalization or basic scaling to 
your data too, such as the following octave code:
price = [1 ([1650 3] - mu) ./ sigma] * theta;


*****  why very different values of features may make the contour skinny and tall :question:
   
***** dividing by maximum value of the feature
      i.g. in training set, the maximum of  possible sizes of houses goes to value
      max_size (max_size > 0). then we could divide the size feature by max_size to 
      make sure its value drop in the range 

***** mean normalization
      $x_{j} := \frac{ x_{j} - avg}{max - min}$ to make sure feature have approximately zero mean : $-0.5 <= x_{j} <= 0.5$
      (do not apply to $x_{0}=1$)

      (max-min) is just alternate here. In fact, we should use standard divation here as:
      $x_{j} = \frac{x_{j} - mean(x_{j})}{std(x_{j})}$, most data points will lie within ±2 standard deviations of the mean); 
      
**** "Debugging"
     to check whether gradient descent is working correctly

***** plot (num of iterations, min(J(theta)))
      we could plot (num of iterations, min(J(theta))) to check out whether min(J) convergence.
      J(theta) should decrease after every iteration
      
      besides plotting, we could employ automatic convergence test in such way
      that declare convergence when J(theta) decreases by less than a specific
      constant (i.g. 0.001). However it's hard to decide such a constant, thus
      plotting way comes to more guaranteed  [[file:r/0180.jpg][more detail]].
      
**** learning rate choosing
     As we discuss before, learning rate need to be sufficiently small to guarantee
     gradient descend works in a correct way.
     With not proper learning rate, we could "debugging" to reveal it. 
     [[file:r/0190.jpg][detail]]
***** summary to choose a proper learning rate.
      + too small a:
	slow convergence 
      + too large:
	J(theta) may not decrease on every iteration; may not converge
	
      to choose a porper one, try such order 0.001 0.003 0.01 0.03 0.1.. (3x scale)

*** polynomial regression
    Sometimes polynomial regression model is more appropriate to your problem than
    the linear regression model(fit your training set more), and we could treat the 
    polynomial regression model as linear reg ression model after some "modification".
    In polynomial regression with one variable model, we may choose the hypothesis:
    + $h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^{2}$ (quadratic function)      
      
      \begin{align}
      h(\theta) & = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_2 \\
      x_{1}\    & = x \\
      x_{2}  & = x^{2}
      \end{align}

    + $h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^{2} + \theta_{3}x^{3}$ (cubic function)

      \begin{align}
      & h(\theta)  = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_2 + \theta_{3}x_{3}\\
      & x_{1} = x \\
      & x_{2} = x^{2} \\
      & x_{3} = x^{3}
      \end{align}
      
*** normal equation for linear regression

In training set, we have $x^{i} = \begin{vmatrix} x_{0}^{i} \\ x_{1}^{i} \\ x_{2}^{i} \\ . \\ .\\ .\\ x_{n}^{i}\end{vmatrix}$  (i-th row) , $y = \begin{vmatrix} y^{1} \\ y^{2} \\ .\\ .\\ .\\ y^{m} \end{vmatrix}$  $X = \begin{vmatrix}(x^{1})^{T} \\ (x^{2})^{T} \\ .\\ .\\ .\\ (x^{m})^{T} \end{vmatrix}$  (m * (n+1) dimension)
then we could formulate $\theta = (X^TX)^{-1}X^Ty$

(tips: X has m rows and n+1 columns (+1 because of the x0=1 term). y is an m-vector
θ is an (n+1)-vector)

**** how it comes

    \begin{align}
    & Our Goal: \\
    & X\theta = y \\
    & X^{T}X\theta = X^{T}y
    & (X^{T}X)^{-1}(XT)
    \end{align}
     
**** the distinction between normal equation and gradient descent algoritmn

|       gradient descent          |           normal equation          |
|---------------------------------+------------------------------------|
| need to choose a                | no need to choose a                |
| needs many iteration            | no need to iterate                 |
| need feature scaling            | need to matrix operatiion          |
| works well even when n is large | slow if n is very large (n > 10^5) |
|                                 |                                    |

**** what if $X^{T}X$ is non-invertible(singular)?
     + redundant features
       e.g 
       \begin{align}
       &x_{1} = size\ in\ feet^{2} \\
       &x_{2} = size\ in\ m^{2}
       \end{align}

     + too many features (e.gl m <= n)
       solution:
       — delete some features
       - use regularization


     
** Logistic Regression (classification)
   focusing binary classification
   Examples of classification:
   + Emal: spam / not spam
   + Online Transactions: Fraudulent(yes/no)
   + Tumor: Malignant/Benign
 
*** Hypothesis Definition
X2
|..
|  .. 	   x   	x    x
|    ..     x	   x
|      ..    x 	x x   x
|	 ..      x
|   	   .. 	 x     	x
|   o	  o  ..       x  x
|     o	       ..      	 x
|   o  	 o    o	 ..    x
|      o     o 	   ..
|    o 	   o   o     .. x
|		       ..
+-------------------------------- X1
    for example A
   
    Notice that different training set, we have merely different H(see non-linear
    decision boundaries)

    Obviously, we could divide(classify) the training set for the example A
    into two groups(y = 0 | y = 1). by a line called *Decision Boundary*.
       
    \begin{align}
    & h_{\theta} = g(\theta^{T}X) \\
    & g(z) = \frac{1}{1 + e^{-z}}
    \end{align}
   
    It's a very neat way to quantize the problem, that's because:
    for one specific theta, and we try to predict y with a array of X,
    the value of H turns the probability of y = 1 under such (theta, X) i.e.
    if $\theta^{T}X > 0$ (maybe equals 5), then H > 0.5 (maybe equals 0.7), we could
    say X have 70% to be y = 1(i.e. maglignant tumor)
    if $\theta^{T}X < 0$ (maybe equals -10), then H < 0.5 (maybe equals 0.2), we could
    say X just have 20& to be y = 1(i.e. more like benign tumor than maglignant.
	 
    So we have the pretty good formula to predict which of concrete value a X goes
    to be and our most concern now is to determine the best theta by minimizing the 
    cost function.

**** the non-linear decision boundary

     		       	     |
		       	     |
		         x   | x
		   x       x |	 x   x
		      x ...........
		  x  ...  o  | o   ... x
       	       	x   .   o   o|  o  o  .
     --------------.---------+---------.----------------
		x   .     o o|   o    .	x
       	       	  x  ...o   o| 	   ...	 x
		   x    ...........    x
		    x  x  x  |	   x
		      x      | 	x
			     |
			     |
			     |
			     
     In this example: we may modify the H to:
     \begin{align}     
     &h_{\theta} = g(\theta_0 x_0 + \theta_1 x_1 + \theta_2  x_2  + \theta_3  x_1 ^ 2 + \theta_4 \times x_2 ^ 2 ....)    \\
     &g(z) = \frac{1}{1 + e^{-z}}
     \end{align}
     then we could transform high-order to linear by supposing like that:
     \begin{align}
     x_3 &= x_1 ^ 2 \\
     x_4 &= x_2 ^ 2 \\
     &maybe: \\
     x_{5} &= x_{1} ^ 3\\
     ... 
     \end{align}
     
*** the cost function of H
    tips: notice that cost function tries to quantize how the H fit the y
    under the same group of x in the training set.

    To find a function to describe the "cost" of H is not easy.
    We couldnt use the square error function as cost function since J(theta)
    may goes to be:      .Obviouly, we could not use gradient descent to find
    local opmitmum becaus it's not the "convex" shape such as the right one 
    as the situation in linear regression.     
    However such cost function is effective:

    $J(\theta)  = \frac{1}{m}\sum_{i=1}^{m} Cost(h_{\theta}(x^{(i)}), y^{(i)})$

    For every $x^{(i)}$ we could compulate its corresponding cost, and then sum up all
    cost to get J. For each cost we have :

    $Cost(h_{\theta}(x^{(i)}), y^{(i)}) = -log(h_{\theta}(x)) \ \ if \ y^{(i)} = 1$
    
    the corresponding cost (not the sum)
    ^
    |
    |.
    | ..
    |  ..
    |	...
    |	  ...
    |	    ...
    |	      ...
    |		 ....
    |		     .........
    |			     ......
    +-----------------------------......|--->  $h_\theta(x)$
    0 			         	y = 1
    if h predict y closes to 1 (the correct value) , then cost will close to zero, meaning
    that it fits very much. On the contrary, the cost will be very large if h predicts value
    close to 0, meaning that it's bad prediction.

    as the same token:
    
    $Cost(h_{\theta}(x^{(i)}), y^{(i)}) = -log(1 - h_{\theta}(x)) \ \ if \ y^{(i)} = 0$


                                        .|
    cost                                .|
    ^				      ...|
    |				     ... |
    |				    ..	 |
    |				   ..	 |
    |				 ...     |
    |			       ...       |
    |			    . .	         |
    |			...	         |
    |		    ...	.	         |
    |	       .....			 |
    |..........				 |
    +------------------------------------+----->
    0					 | 1     $h_\theta(x)$

    So, band them together, we have:
    \begin{align}
    J(\theta)  &= \frac{1}{m}\sum_{i=1}^{m} Cost(h_{\theta}(x^{(i)}), y^{(i)}) \\
    &= -\frac{1}{m}[\sum_{i=1}^{m}y^{i}logh_{\theta}(x^{(i)} + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})]
    \end{align}
    
*** min J to get theta vector (gradient descent)
    We could apply gradient descent algorithm to get local optimal for J of logistic regression.

    $\theta_j := \theta_j - \alpha \times \frac{\partial J(\theta)}{\partial \theta_{j}} = \theta_j - \frac{\alpha}{m} \sum_{i=1}^m(h_{\theta}(x^{i}) - y^i)x_{j}^{(i)} \ \  (0 < j < n)$ 
    
    it's very suprising that after simplifing the partial derivative we get the *same formula* as
    the linear regression.
    Note that while this gradient looks identical to the linear regression gra- dient, the formula
    is actually different because linear and logistic regression have different definitions of hθ(x).

*** logistic regression implemention in octave

    we call $\frac{\partial J(\theta)}{\partial \theta_{j}}$  the gradient for $\theta_j$ .
    Actually just given $J_\theta$ and $\frac{\partial J(\theta)}{\partial \theta_{j}}$ for every $\theta_j$ , then we could use the following 
    algorithms to get min(J) except just the gradient descent alogrithm.

    | algorithm(J, graidient)          | advantages                       |
    |----------------------------------+----------------------------------|
    | Gradient descent                 | simple                           |
    | Conjugate gradient  BFGS  L-BFGS | no need to choose alpha manually |
    |                                  | and always faster than gradient  |
    |                                  | descent.                         |

**** just provide J, gradient in cost function and let octave do the rest.
     Notice that in linear regression exercise, we set learning rate and then do
     gradient descent steps manually. 
     However we could just provide J definition and $\frac{\partial J(\theta)}{\partial \theta_{j}}$ gradient vector in cost
     function to octave and let it do gradient descent steps automatically without
     choosing learning rate manually and return best theta vector.
     revise here
#+begin_src octave
function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));
%
hx = sigmoid(X * theta);
J = (1/m) .* ( transpose(-y) * log(hx)  -  transpose(ones(m,1) - y) * log(ones(m,1) - hx) );
error = hx - y;
for _j = 1:size(theta)
  x_j_clm = X(:, _j);  % the k-th column 
  grad(_j) = (1/m) .* transpose(x_j_clm) * error;
end
end

%  Set options for fminunc
options = optimset('GradObj', 'on', 'MaxIter', 400);

%  Run fminunc to obtain the optimal theta
%  This function will return theta and the cost 
[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);

#+end_src

**** prediction after getting the best theta
#+begin_src octave
function p = predict(theta, X)

%   p = PREDICT(theta, X) computes the predictions for X using a 
%   threshold at 0.5 (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)

m = size(X, 1); % Number of training examples

p = zeros(m, 1);

h =  sigmoid(X * theta);
for _x = 1 : m
  if h(_x) >= 0.5
    p(_x) = 1;
  else
    p(_x) = 0;
  end
end
% =========================================================================
end
#+end_src
    


** Regularization for linear/logistic regression    
*** overfitting problem
    if we have *too many features*, the learned hypothesis may fit
    the training set very well(J close to 0), but fail to generize
    reasonal prediction on new example data.

    notice that too many features here may include polinomial features such as:
    $h_{\theta}(x) = \theta_{0} + \theta_{1}x + \theta_{2}x^{2} + \theta_{3}x^{3} + \theta_{4}^{4}x^{4}$  (polynomial formula)
    saying that we have four features


    In [[file:r/0210.jpg][overfitting example in linear regression]] , the third h overfits
    the training set since we may have unreasonable prediction on new
    size even though h fit the training set very well.

    In [[file:r/0220.jpg][overfitting example in logistic regression]].

    The fact:
    Adding many new features gives us more expressive models which are able to better fit
    our training set. If too many new features are added, this can lead to overfitting of
    the training set.    
    
*** avoiding overfitting
    + reduce number of features
      - manually select which features to keep and throw away
      - model selection algorithm

    + regularization 
      advantages: keep all the features, but reduce magnitude of theta
      , meaning that the reduced theta turns to be relatively small and some theta 
      may reduce to be close to zero thus they just contributes a little bit to
      predict y (equivalent to throw away these features to simplify h but fit training set
      well too) .e.g. [[file:r/0210.jpg][overfitting example in linear regression]] 
      After regularization, theta3, theta4 turns to very small, then h is close
      to the second h.

*** regularization implementation    
    Concretely, regularization operation turns to be same in both linear regresssion and
    logistic regression.

    $J_{\theta} := J_{\theta} + \lambda \sum_{j=1}^{n}\theta_{j}^{2}$    we don't take $\theta_{0}$ to account here
    
    So minimizing J will gurantee the theta vector keep as small as possible
    (maybe some theta wil be close to 0)
    
    So the gradient descent step goes to be, after taking partial derivative,

    $\theta_0 := \theta_0 - \alpha \times \frac{\partial J(\theta)}{\partial \theta_{j}} = \theta_0 - \frac{\alpha}{m} \sum_{i=1}^m(h_{\theta}(x^{i}) - y^i)x_{0}^{(i)}$
    $\theta_j := \theta_j(1- \alpha \frac{\lambda}{m}) - \alpha \times \frac{\partial J(\theta)}{\partial \theta_{j}} = \theta_j(1- \alpha \frac{\lambda}{m}) - \frac{\alpha}{m} \sum_{i=1}^m(h_{\theta}(x^{i}) - y^i)x_{j}^{(i)} \ \  (1 < j < n)$ 
    
    tips: If we introduce too much regularization, we can underfit the training set and this
    can lead to worse performance even for examples not in the training set.    

    revised here, code
**** normal equation regularization for linear regression
     As we known,  we could also apply normal equation to get "best" theta and then
     the regularization for normal equation goes to:
     $(X^{T}X + \lambda \begin{vmatrix} 0 \ \ \ \ \\ \ 1 \ \ \\ \ \ 1 \\ \ \ \ . \end{vmatrix})^{-1}X^{T}y$

     
** multi-class classification
    + Weather : Sunny, Cloudy, Rain, Snow
    + Medical diagrams : Not ill, Cold, Flu
    + Email tagging: Work, Friends, Family, Hobby  

***** solution
      Train a logistic regression classifier h for each class i to predict 
      the probability that y = i ([[file:r/0200.jpg][one-vs-rest]]), if there are n classifier,
      when there are n hypothesis function.
      On a new input x to make a prediction, pick the class i
      that maxmizes $h_\theta^{(i)}(x)$

*** Number recognization exmaple 
    combine multi-class classification (one vs all) and regularization strategies
    together.


** Neutral network
*** The limitatin on non-linear hypothesis
    An Computer vision example: recognizing the image(50*50 pixels) of the car.
    Our standard supervised learning procedure on this problem may be:
    [[./r/0230.jpg]]
    + Building the training set:
      the features or variables, we define , are the grascale values(0~250) of the
      50*50 pixels of the image. And it's obvious linear decision boundary dont fit
      the training set (concretely, we deal with more non-linear decision boundary
      examples than linear ones). So we may try to take quardic items of these 2500
      variables. then we could get O(2500 ^ 2) (2500 * 2500 / 2) features, and
      O(2500 ^3) if we take quadratic items.     
      *quardic items of (x1, x2, x3, x4): x1^2, x1x2, x1x3, x1x4, x2^2, x2*x3 ...... x4^2*

    + Implement our learning algorithm: 

      Get the "best" non-linear hypothesis H.

    + Predictiction:

      Predcit a new image whether it's a car or not based on our learning algorithm.
 
    So the problem when building trianing set surfaces, we have
    *too many variables  which are hard for stroring and computing*,
    which is the biggest limitation on such kind of problems.

*** the neural network model reprentation    
    All kinds of neurons in human's body connect each other in a complex way
    to bulid the neural network. 
    
    [[./r/0240.jpg]]    

    [[./r/0241.jpg]]    

    [[./r/0245.jpg]]
    
    [[./r/0250.jpg]]

    Concretely, we could apply vectorization into it:
    
    $A^{i} = g(\theta * A^{i-1})$
    
    Mode Tips: Actually for each laye, units are unlimited since $\theta$ will map it
    to the next level (*dont forget the bias unit(always equals 1) in each layer*)

*** Examples of neural network    
    #+ATTR_HTML: :width 800px
    [[./r/0260.jpg]]

    #+ATTR_HTML: :width 800px
    [[./r/0270.jpg]]

    #+ATTR_HTML: :width 800px
    [[./r/0280.jpg]]

    then we could put smaller network to build a big one

    #+ATTR_HTML: :width 800px
    [[./r/0290.jpg]]
    
    notice that the problems here,in fact, are non-linear regression to predict
    logistic expression as opposed to logistic regresssion(classification) since
    there is no boundary descision and we just discover a hypothesis to predict
    the logistic expression.

*** Multi-class classification example (digitial recognization)
**** Build the training set
     There are 5000 training examples in ex3data1.mat, where each 
     training example is a 20 pixel by 20 pixel grayscale image of
     the digit. Each pixel is represented by a floating point number
     indicating the grayscale intensity at that location. The 20 by
     20 grid of pixels is “unrolled” into a 400-dimensional vector.
     Each of these training examples becomes a single row in our data
     matrix X. This gives us a 5000 by 400 matrix X where every row is
     a training example for a handwritten digit image.
     The second part of the training set is a 5000-dimensional vector y
     that contains labels for the training set. To make things more compatible
     with Octave/Matlab indexing, where there is no zero index, we have mapped
     the digit zero to the value ten. Therefore, a "0" digit is labeled as "10",
     while the digits “1” to “9” are labeled as “1” to “9” in their natural order.

    


    
     



  If a neural network is overfitting the data, one solution would be to increase
  the regularization parameter λ. A larger value of λ will shrink the magnitude 
  of the parameters Θ, thereby reducing the chance of overfitting the data.


** Advices for applying machine learning			   :noexport:
*** Model selection and train/validate/test sets
    + Generalization error

      refers to how well a learning machine(or hypothesis) generalize on 
      unseen datasets, we usually need to estimiate this value. 

    + Generalization error < training error
      
      [[file:r/0320.jpg]]
    
      Different hypothesis(model) will results in different generalization
      error on unseen data set. Let us suppose we need select one "best" model
      from different polynomial expression of one variable(maybe more in other examples)
      , i.e. we need find the degree(D) that has best generalization error. And then
      we also need to estimiate the generalization error. The procedure goes to be:      

      1. Divide all training set into three parts

	 [[file:r/0323.jpg]]
	 
	 The cost function for first two sets are neccessary for selecting models
	 and cost function of the thrid set is for estimating g-error.

         [[file:r/0322.jpg]] 


      2. Train the hypothesis for different model
	 
	 Minimalize the $J_{train}(\theta)$ on training set to get $\theta$ vector of different models

      3. The best model has the minimal value of $J_{cv}(\theta)$
	 
	 Then, we test the hypothesis' performances from different models 
         on validation set. Obviously, the best fit on validation set has
         the minimal value of $J_{cv}(\theta)$.

	 [[file:r/0321.jpg]]

      4. Calculate the generalization error(g-error)
	 
	 Now we have our "best" model, then we could calcualte $J_{test}(\theta)$ as g-error
         on our test set.
        
*** Diagnosing bias and variance problem
    High bias --> underfit
    High variance --> overfit 
	
    [[file:r/0328.jpg]]
    
    It's straightforward to diagnose the bias and variance problem from
    $J_{train}, \ J_{cv}$, all details in the following figure.

    file:r/0329.jpg
        
*** Regularization helps to select model
    (My assumption here, may uncorrect)

    Model selection introduced in previous heading is unaccessible
    if we get too many variables and thus too many hypothesis model
    we have to "compare"(even consider the polynomial expression of all 
    variables).Our chance here is try to determine a specific hypothesis
    (must be overfitting the trainning set)  and then introduce regularization
    to eliminate some theta thus we have pretty "right" hypothesis model.
    As we known, different values of lambda for regularization may lead to 
    different hypothesis model. E.g. too large lambda will make h underfit
    too small will make h overfit. So our question is how to choose the 
    value of lambda to make our hypothesis "shrink" to the right one.
    The next talks about the strategy to choose lambda.
    
    Suppose we have determined our hypothesis model:

    $h(\theta) = \theta_{0} + \theta_{1}x^{2} + \theta_{2}x^{3} + \theta_{4}x^{4} + \theta_{5}x^{5}$    
    

    file:r/0324.jpg

    Our strategy here is similar to model selection introduced in previous heading:

    file:r/0325.jpg

    [[file:r/0326.jpg]]

    Concretely, we could plot how the range of lambda effects $J_{train}, \ J_{cv}$
    file:r/0327.jpg
    
    The foundamental rule about regularization is that,
    regularization must build on a specific and determined hypothesis.


*** Learning curve
    We could plot how m(triaing set size) effects  $J_{train}, \ J_{cv}$, the plot
    curve is what we called learning curve, which could also help us to understand
    if there is a high bias or high variance problem.

    + Learning curve of high-bias situation

      [[file:r/0330.jpg]]

    + Learning curve of high-variance situation
      
      [[file:r/0331.jpg]]
    
*** Debugging a learning algorithm:
    Suppose you have implemented regularized linear regression to predict housing prices.
    However, when you test your hypothesis in a new set of houses,
    you find that it makes unacceptably large errors in its predicDon
    . What should you try next?

    + Get more training examples --> fixes high variance
    + Try smaller sets of features --> fixes high variance
    + Try get more features --> fixes high bias
    + Try adding polynomial features --> fixes high bias
    + Try decreasing lambda for regularization --> fix high bias
    + Try increasing lambdas --> fix high variance


*** High bias and high variance in neural network

    
** Machine learning system design procedure			   :noexport:
*** Prioritize what to do on design ml system (spam/non-span emails example)    
**** Building a spam classifier    
     [[file:r/0332.jpg]]
     
     file:r/0333.jpg

     file:r/0334.jpg

**** Error analysis
***** error metric for ml algorithm
      *Basically, we take $J_{cv}$ as our error metric (except skewed
      classes) to measure how this temporary algorithm do well when we are try to improve
      our algorithm*. Notice that $J_{test}$ are measured for generalization
      error (g-error is measured when algorithm has ultimately 
      determined). Error metric decline after taking some solutons probably 
      means it's a good idea to take them. E.g. we use error metric to guide us do
      model selection and do stemming for the spam email classifier 
      described in following example, etc.

     file:r/0335.jpg
     
     file:r/0336.jpg

     Thus, building a quick and dirty prototype and do error analysis
     could help u figure out what solution(add some more features) may
     be taken to improve the performance. But the only way to see
     if our solution works is to try it and compute error metric 
     (basically, $J_{cv}$) and then check whether the value has declined 
     as opposed to previous version without such solution since our solution
     may help our algorithm but hurt some parts of it at the same time.
     , for example:

     file:r/0337.jpg

     Notice that u really need implement your system as
     quickly as you can, and once you have the initial implementation this
     is then a powerful tool for deciding where to spend your
     time next, because first we can look at the errors it makes,
     and do this sort of error analysis to see what mistakes it makes
     and use that to inspire further development. And second,
     assuming your quick and dirty implementation incorporated a
     single real number error metric(suppose $J_{cv}$, this
     can then be a vehicle for you to try out different ideas
     and quickly see if the different ideas you're trying out
     are improving the performance of your algorithm and therefore let
     you maybe much more quickly make decisions about what things
     to fold, and what things to incorporate into your learning algorithm.
     
     Why is the recommended approach to perform error analysis using the cross
     validation data used to compute $J_{cv}(\theta)$ rather than the test data used to
     compute $J_{test}(\theta)$?

     If we develop new features by examining the test set, then we may end up choosing features
     that work well specifically for the test set, so $J_{test}$ is no longer a good estimate of
     how well we generalize to new examples (g-error).

**** Evaluation error metric for skewed classes
     $J_{cv}$ maybe pretty perfect as error metric unless the number of one class 
     and the other class goes to two extremes in classification (skewed classes)
     , for example:

     file:r/0338.jpg
***** Precision/Recall as error metric for skewed classes
      
      file:r/0339.jpg
      
      It's easier to remember the notations here, (a b) means our prediction is b and
      it's a?correct:uncorrect. E.g. (true positives) means the number of the data we
      corrrectly predict positives.
           
      Concretely ,accuracy = (true positives + true negatives) / (total examples) is what
      we evaluate for non-skewwed classes.

      If you always predict non-spam (output y=0), your classifier will have a recall of 0%.

      If you always predict spam (output y=1), your classifier will have a recall of 100% and precision of 1%.

      A good classifier should have both a high precision and high recall on the cross validation set.

***** Trade off precision and recall

      file:r/0341.jpg

      Thus, high precision and high recall of algorithm is all we asked, and we
      need a formular(score) to measure it. 

      file:r/0340.jpg

**** When trianing set size improve learning algorithm
     
     file:r/0342.jpg

     Notice that more training set won't help on your determined algorithm for any situations.

     *determined algorithm* means the hypothesis form are determined for regression, E.g.
     $h = \theta_{0} x + \theta_{1} x^{2} + \theta_{2} x^{3} + \theta_{3} x^{4}$

     for N-N, we mean we have a specific determined architecture form.
     file:r/0343.jpg
    

     If the performance of a *determined* algorithm keeps unexpected(high test error $J_{test}$)
     Collecting more traingset may help when two of the following conditions hold true
     , *otherwise collecting more data is time-wasting*:
     + We already have sufficient features/variables
       If we have not enough features then even though we get massive
       training we obviously could not get a high accuracy algorithm.
       Eg. predict house  prices with only the size feature.
       (Millions of data won't help)
       
       And we could test whether we have enough feature by asking ourselves:
       Given the input x, can a human expert confidently predict y?
       (It's actually a certificaton we could predict y from out input x)

     + Our alogrithm is already fairly complex
       A sufficiently complex leaning algorithm has the ability to describe well the 
       characteristic of the training set, (e.g. logistic regresssion/linear regression 
       with polynomial parameter; neural network with many hidden units).Even though we
       have enough features, insufficient learning algorithm may lead fit the dataset
       poorly no matter how many training data we have.
                   
     Concretely, both the two conditions guarantee to avoid underfitting problem 
     ($J_{train}(\theta)$ will be low) and more training set helps our determined algorithm "realizes"
     the characteristic and tendency much better and then fit them more reasonablly
     (learn the data more better).And more dataset could also help avoid overfitting
     problem. So, we have the great foundation to meet $J_{train}(\theta)$ ≈  $J_{test}(\theta)$ = a
     relatively low value.



